{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYPAWyXv-QKs"
   },
   "source": [
    "# Image Captioning Final Project\n",
    "\n",
    "In this final project you will define and train an image-to-caption model, that can produce descriptions for real world images!\n",
    "\n",
    "<img src=\"images/encoder_decoder.png\" style=\"width:70%\">\n",
    "\n",
    "Model architecture: CNN encoder and RNN decoder. \n",
    "(https://research.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qirwbzmn-QK5"
   },
   "source": [
    "Alright, here's our plan:\n",
    "* Take a pre-trained inception v3 to vectorize images\n",
    "* Stack an LSTM on top of it\n",
    "* Train the thing on MSCOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "shred -u setup_colab.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/hse-aml/intro-to-dl-pytorch/main/utils/setup_colab.py -O setup_colab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup_colab\n",
    "\n",
    "setup_colab.setup_week06()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grading\n",
    "import grading_utils\n",
    "\n",
    "grader = grading.Grader(\n",
    "    assignment_key=\"NEDBg6CgEee8nQ6uE8a7OA-hZg\",\n",
    "    all_parts=[\"19Wpv\", \"E2OIL\", \"rbpnH\", \"YJR7z\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = ### YOUR TOKEN HERE\n",
    "COURSERA_EMAIL = ### YOUR EMAIL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3maVO6q-QK7"
   },
   "outputs": [],
   "source": [
    "# We will take images that have already been processed to save time\n",
    "# so, please download the data from https://yadi.sk/d/b4nAwIE73TVcp5\n",
    "\n",
    "# You can use https://github.com/wldhx/yadisk-direct like\n",
    "# !pip3 install wldhx.yadisk-direct\n",
    "# !curl -L $(yadisk-direct https://yadi.sk/d/b4nAwIE73TVcp5) -o data.tar.gz\n",
    "# !tar -xvf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 950,
     "status": "ok",
     "timestamp": 1613315463748,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "x_wH1bYu-QK8",
    "outputId": "f7c4eafc-3c70-4f3e-d83b-b6ade7f38712"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load dataset (vectorized images and captions)\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "img_codes = np.load(\"data/image_codes.npy\")\n",
    "captions = json.load(open('data/captions_tokenized.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 636,
     "status": "ok",
     "timestamp": 1613315464076,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "VR8TcbYN-QK-",
    "outputId": "5406c930-ef51-4abe-ceab-b219e3a99e38"
   },
   "outputs": [],
   "source": [
    "print(\"Each image code is a 2048-unit vector [ shape: %s ]\" % str(img_codes.shape))\n",
    "print(img_codes[0,:10], end='\\n\\n')\n",
    "print(\"For each image there are 5 reference captions, e.g.:\\n\")\n",
    "print('\\n'.join(captions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKiDzdJ0-QK-"
   },
   "source": [
    "As you can see, all captions are already tokenized and lowercased. We now want to split them and add some special tokens for start/end of caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo6T5a0O-QK_"
   },
   "outputs": [],
   "source": [
    "# split descriptions into tokens\n",
    "for img_i in range(len(captions)):\n",
    "    for caption_i in range(len(captions[img_i])):\n",
    "        sentence = captions[img_i][caption_i] \n",
    "        captions[img_i][caption_i] = [\"#START#\"] + sentence.split(' ') + [\"#END#\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz_1Nbm4-QK_"
   },
   "source": [
    "You don't want your network to predict a million-size vector of probabilities at each step, so we're gotta make some cuts.\n",
    "\n",
    "We want you to count the occurences of each word so that we can decide which words to keep in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF_TT8zs-QK_"
   },
   "outputs": [],
   "source": [
    "# Build a Vocabulary\n",
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "\n",
    "# Compute word frequencies for each word in captions. See code above for data structure\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qdU79Qk-QLA"
   },
   "outputs": [],
   "source": [
    "vocab  = ['#UNK#', '#START#', '#END#', '#PAD#']\n",
    "vocab += [k for k, v in word_counts.items() if v >= 5 if k not in vocab]\n",
    "n_tokens = len(vocab)\n",
    "\n",
    "assert 10000 <= n_tokens <= 10500\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"19Wpv\", grading_utils.test_vocab(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EADUtf5A-QLA"
   },
   "outputs": [],
   "source": [
    "eos_ix = word_to_index['#END#']\n",
    "unk_ix = word_to_index['#UNK#']\n",
    "pad_ix = word_to_index['#PAD#']\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + pad_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word, unk_ix) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1613315600593,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "YDtJMwcl-QLA",
    "outputId": "7e894941-743f-4027-f553-6081a9fe5495"
   },
   "outputs": [],
   "source": [
    "# Try it out on several descriptions of a random image\n",
    "as_matrix(captions[1337])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnzoqZIf-QLB"
   },
   "source": [
    "### Building our neural network\n",
    "As we mentioned earlier, we shall build an rnn \"language-model\" conditioned on vectors from the convolutional part.\n",
    "\n",
    "<img src=\"https://github.com/yunjey/pytorch-tutorial/raw/master/tutorials/03-advanced/image_captioning/png/model.png\" style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCHp6va--QLB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBKB5DPR-QLB"
   },
   "outputs": [],
   "source": [
    "class CaptionNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_tokens=n_tokens, emb_size=128, lstm_units=256, cnn_feature_size=2048):\n",
    "        \"\"\" A recurrent 'head' network for image captioning. See scheme above. \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # a layer that converts conv features to initial_h (h_0) and initial_c (c_0)\n",
    "        self.cnn_to_h0 = nn.Linear(cnn_feature_size, lstm_units)\n",
    "        self.cnn_to_c0 = nn.Linear(cnn_feature_size, lstm_units)\n",
    "\n",
    "        # create embedding for input words. Use the parameters (e.g. emb_size).\n",
    "        # YOUR CODE HERE\n",
    "            \n",
    "        # lstm: create a recurrent core of your network. Use either LSTMCell or just LSTM. \n",
    "        # In the latter case (nn.LSTM), make sure batch_first=True\n",
    "        # YOUR CODE HERE\n",
    "            \n",
    "        # create logits: linear layer that takes lstm hidden state as input and computes one number per token\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, image_vectors, captions_ix):\n",
    "        \"\"\" \n",
    "        Apply the network in training mode. \n",
    "        :param image_vectors: torch tensor containing inception vectors. shape: [batch, cnn_feature_size]\n",
    "        :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n",
    "            padded with pad_ix\n",
    "        :returns: logits for next token at each tick, shape: [batch, word_i, n_tokens]\n",
    "        \"\"\"\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        initial_cell = self.cnn_to_c0(image_vectors)\n",
    "        initial_hid = self.cnn_to_h0(image_vectors)\n",
    "        \n",
    "        # compute embeddings for captions_ix\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # apply recurrent layer to captions_emb. \n",
    "        # 1. initialize lstm state with initial_* from above\n",
    "        # 2. feed it with captions. Mind the dimension order in docstring\n",
    "        # 3. compute logits for next token probabilities\n",
    "        # Note: if you used nn.LSTM, you can just give it (initial_cell[None], initial_hid[None]) as second arg\n",
    "\n",
    "        # lstm_out should be lstm hidden state sequence of shape [batch, caption_length, lstm_units]\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # compute logits from lstm_out\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oBfMNW9-QLC"
   },
   "outputs": [],
   "source": [
    "network = CaptionNet(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1613316480087,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "XfXlXLiV-QLC",
    "outputId": "e7ea62cb-95ed-4efe-ecd7-8e6eedf0f1a2"
   },
   "outputs": [],
   "source": [
    "dummy_img_vec = torch.randn(len(captions[0]), 2048)\n",
    "dummy_capt_ix = torch.tensor(as_matrix(captions[0]), dtype=torch.int64)\n",
    "\n",
    "dummy_logits = network.forward(dummy_img_vec, dummy_capt_ix)\n",
    "\n",
    "print('shape:', dummy_logits.shape)\n",
    "assert dummy_logits.shape == (dummy_capt_ix.shape[0], dummy_capt_ix.shape[1], n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yx68R6Fn-QLD"
   },
   "outputs": [],
   "source": [
    "def compute_loss(network, image_vectors, captions_ix):\n",
    "    \"\"\"\n",
    "    :param image_vectors: torch tensor containing inception vectors. shape: [batch, cnn_feature_size]\n",
    "    :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n",
    "        padded with pad_ix\n",
    "    :returns: crossentropy (neg llh) loss for next captions_ix given previous ones. Scalar float tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # captions for input - all except last because we don't know next token for last one.\n",
    "    captions_ix_inp = captions_ix[:, :-1].contiguous()\n",
    "    captions_ix_next = captions_ix[:, 1:].contiguous()\n",
    "    \n",
    "    # apply the network, get predictions for captions_ix_next\n",
    "    logits_for_next = network.forward(image_vectors, captions_ix_inp)\n",
    "    \n",
    "    # compute the loss function between logits_for_next and captions_ix_next\n",
    "    # Use the mask!\n",
    "    # Make sure that predicting next tokens after EOS do not contribute to loss\n",
    "    # You can do that either by multiplying elementwise loss by (captions_ix_next != pad_ix)\n",
    "    # or by using ignore_index in some losses.\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jDz4fRD-QLD"
   },
   "outputs": [],
   "source": [
    "dummy_loss = compute_loss(network, dummy_img_vec, dummy_capt_ix)\n",
    "\n",
    "assert len(dummy_loss.shape) <= 1, 'loss must be scalar'\n",
    "assert dummy_loss.data.numpy() > 0, \"did you forget the 'negative' part of negative log-likelihood\"\n",
    "\n",
    "dummy_loss.backward()\n",
    "\n",
    "assert all(param.grad is not None for param in network.parameters()), \\\n",
    "    'loss should depend differentiably on all neural network weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"rbpnH\", grading_utils.test_network(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3IDzYtf-QLD"
   },
   "source": [
    "## Training\n",
    "* First implement the batch generator\n",
    "* Than train the network as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2094,
     "status": "ok",
     "timestamp": 1613316838905,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "nGojrEFn-QLD",
    "outputId": "6b9269b0-8ed5-44a2-8482-af4b6c5c09cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "captions = np.array(captions)\n",
    "train_img_codes, val_img_codes, train_captions, val_captions = train_test_split(\n",
    "    img_codes, captions, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiYPJTpK-QLE"
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def generate_batch(img_codes, captions, batch_size, max_caption_len=None):\n",
    "    \n",
    "    # sample random numbers for image/caption indicies\n",
    "    random_image_ix = np.random.randint(0, len(img_codes), size=batch_size)\n",
    "    \n",
    "    # get images\n",
    "    batch_images = img_codes[random_image_ix]\n",
    "    \n",
    "    # 5-7 captions for each image\n",
    "    captions_for_batch_images = captions[random_image_ix]\n",
    "    \n",
    "    # pick one from a set of captions for each image\n",
    "    batch_captions = list(map(choice,captions_for_batch_images))\n",
    "    \n",
    "    # convert to matrix\n",
    "    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n",
    "    \n",
    "    return torch.tensor(batch_images, dtype=torch.float32), \\\n",
    "        torch.tensor(batch_captions_ix, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_batch(img_codes, captions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1613317062173,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "OMetX8Qf-QLE",
    "outputId": "5b94a6a5-8fe4-4f12-d557-90e6df6f78b2"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"E2OIL\", grading_utils.test_batch(generate_batch(img_codes, captions, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Klt7Ueuk-QLE"
   },
   "source": [
    "### Main loop\n",
    "Train on minibatches just as usual. Evaluate on val from time to time\n",
    "\n",
    "##### TIps\n",
    "* If training loss has become close to 0 or model produces garbage, double-check that you're predicting next words, not current or t+2'th words\n",
    "* If the model generates fluent captions that have nothing to do with the images\n",
    "    * this may be due to recurrent net not receiving image vectors.\n",
    "    * alternatively it may be caused by gradient explosion, try clipping 'em or just restarting the training\n",
    "    * finally, you may just need to train the model a bit more\n",
    "* Crossentropy is a poor measure of overfitting\n",
    "    * Model can overfit validation crossentropy but keep improving validation quality.\n",
    "    * Use human (manual) evaluation or try automated metrics: cider or bleu\n",
    "    \n",
    "* We recommend you to periodically evaluate the network using the next \"apply trained model\" block\n",
    "    * its safe to interrupt training, run a few examples and start training again\n",
    "* The typical loss values should be around 3~5 if you average over time, scale by length if you sum over time. The reasonable captions began appearing at loss=2.8 ~ 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqfu7NEyMyev"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FicwmmxaNA4n"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsD-vuuP-QLF"
   },
   "outputs": [],
   "source": [
    "network = CaptionNet(n_tokens).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 128  # adjust me\n",
    "n_epochs = 100  # adjust me\n",
    "n_batches_per_epoch = 50  # adjust me\n",
    "n_validation_batches = 5  # how many batches are used for validation after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189202,
     "status": "ok",
     "timestamp": 1613317618908,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "9NNjNlh8-QLF",
    "outputId": "3e9295a7-0cc6-4ff7-f50a-ede71e33643d"
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    network.train()\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        images, captions = generate_batch(train_img_codes, train_captions, batch_size)\n",
    "        images = images.to(DEVICE)\n",
    "        captions = captions.to(DEVICE)\n",
    "\n",
    "        loss_t = compute_loss(network, images, captions)\n",
    "        \n",
    "        # clear old gradients; do a backward pass to get new gradients; then train with opt\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        train_loss += loss_t.detach().cpu().numpy()\n",
    "        \n",
    "    train_loss /= n_batches_per_epoch\n",
    "    \n",
    "    val_loss = 0\n",
    "    network.eval()\n",
    "    for _ in range(n_validation_batches):\n",
    "        images, captions = generate_batch(train_img_codes, train_captions, batch_size)\n",
    "        images = images.to(DEVICE)\n",
    "        captions = captions.to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_t = compute_loss(network, images, captions)\n",
    "\n",
    "        val_loss += loss_t.detach().cpu().numpy()\n",
    "\n",
    "    val_loss /= n_validation_batches\n",
    "    \n",
    "    clear_output()\n",
    "    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"YJR7z\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply trained model\n",
    "\n",
    "Let's unpack our pre-trained inception network and see what our model is capable of!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beheaded_inception3 import beheaded_inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = beheaded_inception_v3().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqCMkyZJ-QLI"
   },
   "source": [
    "## Generate caption\n",
    "The function below creates captions by sampling from probabilities defined by the net.\n",
    "\n",
    "The implementation used here is simple but inefficient (quadratic in lstm steps). We keep it that way since it isn't a performance bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bXJSUnD-QLJ"
   },
   "outputs": [],
   "source": [
    "network = network.cpu().eval()\n",
    "\n",
    "def generate_caption(image, caption_prefix = (\"#START#\",), \n",
    "                     t=1, sample=True, max_len=100):\n",
    "    \n",
    "    assert isinstance(image, np.ndarray) and np.max(image) <= 1\\\n",
    "           and np.min(image) >=0 and image.shape[-1] == 3\n",
    "    \n",
    "    image = torch.tensor(image.transpose([2, 0, 1]), dtype=torch.float32)\n",
    "    \n",
    "    vectors_8x8, vectors_neck, logits = inception(image[None])\n",
    "    caption_prefix = list(caption_prefix)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        prefix_ix = as_matrix([caption_prefix])\n",
    "        prefix_ix = torch.tensor(prefix_ix, dtype=torch.int64)\n",
    "        next_word_logits = network.forward(vectors_neck, prefix_ix)[0, -1]\n",
    "        next_word_probs = F.softmax(next_word_logits, -1).detach().numpy()\n",
    "        \n",
    "        \n",
    "        assert len(next_word_probs.shape) ==1, 'probs must be one-dimensional'\n",
    "        next_word_probs = next_word_probs ** t / np.sum(next_word_probs ** t) # apply temperature\n",
    "\n",
    "        if sample:\n",
    "            next_word = np.random.choice(vocab, p=next_word_probs) \n",
    "        else:\n",
    "            next_word = vocab[np.argmax(next_word_probs)]\n",
    "\n",
    "        caption_prefix.append(next_word)\n",
    "\n",
    "        if next_word==\"#END#\":\n",
    "            break\n",
    "            \n",
    "    return caption_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2538,
     "status": "ok",
     "timestamp": 1613319002844,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "tMPXvS5G-QLJ",
    "outputId": "fc28584f-0b58-4904-991b-ab8c8980658c"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from skimage.transform import resize\n",
    "%matplotlib inline\n",
    "\n",
    "#sample image\n",
    "!wget https://avatars.mds.yandex.net/get-zen_doc/1578906/pub_5deb7d24a06eaf00af983448_5deb7d6643863f00ae06dc24/scale_1200 -O data/img.jpg\n",
    "img = plt.imread('data/img.jpg')\n",
    "img = resize(img, (299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "executionInfo": {
     "elapsed": 1007,
     "status": "ok",
     "timestamp": 1613319004834,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "C4wjQYwE-QLJ",
    "outputId": "67bbf53d-af7b-425c-b553-d0c92984b6f3"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4174,
     "status": "ok",
     "timestamp": 1613319011414,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "2WLG7Wae-QLJ",
    "outputId": "5cca03c6-9e41-4460-b72d-01fd7b6039d6"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(' '.join(generate_caption(img, t=5.)[1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjXdapa8-QLK"
   },
   "source": [
    "# Demo\n",
    "## Find at least 10 images to test it on.\n",
    "* Seriously, that's part of an assignment. Go get at least 10 pictures to get captioned\n",
    "* Make sure it works okay on simple images before going to something more comples\n",
    "* Photos, not animation/3d/drawings, unless you want to train CNN network on anime\n",
    "* Mind the aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRkTrb-E-QLK"
   },
   "outputs": [],
   "source": [
    "### YOUR EXAMPLES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRKCdMxV-QLK"
   },
   "source": [
    "That's it! \n",
    "\n",
    "Congratulations, you've trained your image captioning model and now can produce captions for any picture from the  Internet!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "week6_final_project_image_captioning_clean.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "157px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03cce6abcd9b4a33b7fad11fab0ea4c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_750308835a8f4073828bc0dae7820810",
      "max": 108857766,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_487601952d6843098f4961e92784bba8",
      "value": 108857766
     }
    },
    "288421d1fc8d4a43ade972d36da57f45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "487601952d6843098f4961e92784bba8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "70c5eda26e0a491a80fc2a853fc648db": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74e8b2881eb541d18d028496e3575763": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "750308835a8f4073828bc0dae7820810": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebfd785f70d8437ebaaaffb11dfc89bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03cce6abcd9b4a33b7fad11fab0ea4c2",
       "IPY_MODEL_f4d129166ad645318c51c5e26e41c3b0"
      ],
      "layout": "IPY_MODEL_70c5eda26e0a491a80fc2a853fc648db"
     }
    },
    "f4d129166ad645318c51c5e26e41c3b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74e8b2881eb541d18d028496e3575763",
      "placeholder": "​",
      "style": "IPY_MODEL_288421d1fc8d4a43ade972d36da57f45",
      "value": " 104M/104M [00:00&lt;00:00, 114MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
